"""
Deep Research Bot Expert that uses LangGraph workflow for comprehensive research.
"""
from typing import Any, Dict, Union, List
import ast
import json
import re

from injector import inject
from langchain_core.messages import AIMessage
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from langchain_tavily import TavilySearch

from src.common.config import Config
from src.base.components import MemoryInterface
from src.base.brains import BrainInterface
from src.common.logging import logger
import src.experts.deepresearch_bot.prompts as deepresearch_prompts
from src.experts.deepresearch_bot.utils import (
    get_current_date,
    get_research_topic,
)
from src.experts.deepresearch_bot.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
)
from src.experts.deepresearch_bot.models import WebSearchResult


class SimpleWorkflow:
    """
    Deep Research Expert that uses LangGraph workflow for comprehensive web research.
    """
    
    @inject
    def __init__(
        self,
        config: Config,
        memory: MemoryInterface,
        brain: BrainInterface,
    ):
        """
        Initialize the Deep Research Expert.
        
        Args:
            config: Application configuration
            memory: Memory implementation for storing conversation history
            brain: Brain implementation (not used in this expert as it uses LangGraph)
        """
        self.config = config
        self.memory = memory
        self.brain = brain
        self.graph = self._build_graph()

    def get_initial_state(self, history: List[Dict[str, Any]]) -> OverallState:
        # Create initial state for the workflow
        initial_state: OverallState = {
            "messages": history,
            "search_query": [],
            "web_research_result": [],
            "sources_gathered": [],
            "initial_search_query_count": self.config.number_of_initial_queries,
            "max_research_loops": self.config.max_research_loops,
            "research_loop_count": 0,
        }
        return initial_state

    def _build_graph(self):
        """Build the LangGraph workflow."""
        # Create our Agent Graph
        builder = StateGraph(OverallState)  # type: ignore

        # Define the nodes we will cycle between
        builder.add_node("generate_query", self._generate_query)  # type: ignore
        builder.add_node("web_research", self._web_research)  # type: ignore
        builder.add_node("reflection", self._reflection)  # type: ignore
        builder.add_node("finalize_answer", self._finalize_answer)  # type: ignore

        # Set the entrypoint as `generate_query`
        builder.add_edge(START, "generate_query")
        # Add conditional edge to continue with search queries in a parallel branch
        builder.add_conditional_edges(
            "generate_query", self._continue_to_web_research, ["web_research", "finalize_answer"]  # type: ignore
        )
        # Reflect on the web research
        builder.add_edge("web_research", "reflection")
        # Evaluate the research
        builder.add_conditional_edges(
            "reflection", self._evaluate_research, ["web_research", "finalize_answer"]  # type: ignore
        )
        # Finalize the answer
        builder.add_edge("finalize_answer", END)

        return builder.compile(name="deep-search-agent")  # type: ignore

    def _generate_query(self, state: OverallState, config: RunnableConfig = None) -> QueryGenerationState:  # type: ignore
        """LangGraph node that generates search queries based on the User's question."""

        try:
            # Format the prompt
            current_date = get_current_date()
            formatted_prompt = deepresearch_prompts.query_writer_instructions.format(
                current_date=current_date,
                number_queries=state["initial_search_query_count"],
            )
            
            # Use the LLM client to generate structured output
            response = self.brain.think(history=state["messages"], system_message=formatted_prompt)
            
            # Parse the response to extract queries (simplified for now)
            # In a real implementation, you'd want to use structured output
            content = response.get("content", "")
            match = re.search(r"```json\s*\n*(.*?)\n*```", content, re.DOTALL)
            if match:
                content = match.group(1)
            else:
                raise ValueError(f"No match found for content: {content}")
            try:
                output = ast.literal_eval(content)
            except Exception as e:
                output = json.loads(content)
            
            return {
                "query_list": output["query"][:state["initial_search_query_count"]],
                "do_research": output["do_research"]
                }
        except Exception as e:
            import traceback
            traceback.print_exc()
            logger.error(f"[generate_query node ERROR] {e}", exc_info=True)
            return {
                "query_list": [],
                "do_research": False
            }

    def _continue_to_web_research(self, state: QueryGenerationState):
        """LangGraph node that sends the search queries to the web research node."""
        if state["do_research"] is True:
            return [
                Send("web_research", {"search_query": search_query, "id": str(idx)})
                for idx, search_query in enumerate(state["query_list"])
            ]
        else:
            return "finalize_answer"

    def _web_research(self, state: WebSearchState, config: RunnableConfig = None) -> Dict[str, Any]:  # type: ignore
        """LangGraph node that performs web research using the native Google Search API tool."""
        try:
            # search_tool = CustomSearchTool()
            search_tool = TavilySearch()
            results = search_tool.run(state["search_query"])

            sources_gathered: List[Dict[str, Any]] = []
            snippet_blocks: List[str] = []

            # Tavily returns results in a different format - extract the content
            if isinstance(results, str):
                # If results is a string, it might be the formatted content
                web_research_result = [results] if results.strip() else [WebSearchResult.NO_USEFUL_RESULTS.value]
                return {
                    "sources_gathered": [],
                    "search_query": [state["search_query"]],
                    "web_research_result": [web_research_result],
                }
            
            # Handle Tavily's structured response format
            if isinstance(results, dict):
                organic_results = results.get("results", [])
            elif isinstance(results, list):
                organic_results = results
            else:
                organic_results = []

            for i, result in enumerate(organic_results):
                title = result.get("title", "").strip()
                url = result.get("url", "").strip()  # Tavily uses "url" instead of "link"
                snippet = result.get("content", "").strip()  # Tavily uses "content" instead of "snippet"

                if not title or not url:
                    continue  # skip bad results

                # Define short_url as a numbered citation marker
                short_url = f"[{i + 1}]"
                value = url  # or expand this to dict(url, title) if needed

                sources_gathered.append({
                    "title": title,
                    "short_url": short_url,
                    "value": value,
                    "snippet": snippet,
                })

                snippet_blocks.append(f"{short_url} {title}: {snippet}")

            web_research_result = ["\n".join(snippet_blocks)] if snippet_blocks else [WebSearchResult.NO_USEFUL_RESULTS.value]

            return {
                "sources_gathered": sources_gathered,
                "search_query": [state["search_query"]],
                "web_research_result": [web_research_result],
            }
        except Exception as e:
            import traceback
            traceback.print_exc()
            logger.error(f"[web_research node ERROR] {e}", exc_info=True)
            return {
                "sources_gathered": [],
                "search_query": [state.get("search_query", "unknown")],
                "web_research_result": [[WebSearchResult.ERROR.value]]
            }

    def _reflection(self, state: OverallState, config: RunnableConfig = None) -> ReflectionState:  # type: ignore
        """LangGraph node that identifies knowledge gaps and generates potential follow-up queries."""
        try:
            # Increment the research loop count
            state["research_loop_count"] = state.get("research_loop_count", 0) + 1  # type: ignore

            # Format the prompt
            current_date = get_current_date()
            summaries = "\n\n---\n\n".join(
                [
                    "Query: {query}\n\nSearch Results: {dummy_result}".format(
                        query=query,
                        dummy_result='\n'.join([r for r in result if r not in [WebSearchResult.NO_USEFUL_RESULTS.value, WebSearchResult.ERROR.value]])
                    )
                    for query, result in zip(state["search_query"], state["web_research_result"])
                ]
            )
            formatted_prompt = deepresearch_prompts.reflection_instructions.format(
                current_date=current_date,
                research_topic=get_research_topic(state["messages"]),  # type: ignore
                summaries=summaries,
            )
            
            # Use LLM client for reflection
            response = self.brain.think([{"role": "user", "content": formatted_prompt}])
            
            # Parse response (simplified - in real implementation use structured output)
            content = response.get("content", "")
            match = re.search(r"```json\s*\n*(.*?)\n*```", content, re.DOTALL)
            if match:
                content = match.group(1)
            else:
                raise ValueError(f"No match found for content: {content}")
            
            try:
                output = ast.literal_eval(content)
            except Exception as e:
                output = json.loads(content)
            
            # Simple parsing logic (would be better with structured output)
            is_sufficient = output["is_sufficient"]
            knowledge_gap = output["knowledge_gap"]
            follow_up_queries = output["follow_up_queries"]#[line.strip() for line in lines if line.strip().startswith('-')][:2]

            return {
                "is_sufficient": is_sufficient,
                "knowledge_gap": knowledge_gap,
                "follow_up_queries": follow_up_queries,
                "research_loop_count": state["research_loop_count"],
                "number_of_ran_queries": len(state["search_query"]),  # type: ignore
                }
        except Exception as e:
            import traceback
            traceback.print_exc()
            logger.error(f"[reflection node ERROR] {e}", exc_info=True)
            return {
                "is_sufficient": False,
                "knowledge_gap": "Error in reflection node",
                "follow_up_queries": [],
                "research_loop_count": state["research_loop_count"],
                "number_of_ran_queries": len(state["search_query"]),  # type: ignore
            }

    def _evaluate_research(
        self,
        state: ReflectionState,
        config: RunnableConfig = None,  # type: ignore
    ) -> Union[str, List[Send]]:
        """LangGraph routing function that determines the next step in the research flow."""
        
        if state["is_sufficient"] or state["research_loop_count"] >= self.config.max_research_loops:
            return "finalize_answer"
        else:
            return [
                Send(
                    "web_research",
                    {
                        "search_query": follow_up_query,
                        "id": str(state["number_of_ran_queries"] + int(idx)),
                    },
                )
                for idx, follow_up_query in enumerate(state["follow_up_queries"])  # type: ignore
            ]

    def _finalize_answer(self, state: OverallState, config: RunnableConfig = None) -> Dict[str, Any]:  # type: ignore
        """LangGraph node that finalizes the research summary."""
        # Format the prompt
        current_date = get_current_date()
        summaries = "\n\n---\n\n".join(
            [
                "Query: {query}\n\nSearch Results: {dummy_result}".format(
                    query=query,
                    dummy_result='\n'.join([r for r in result if r not in [WebSearchResult.NO_USEFUL_RESULTS.value, WebSearchResult.ERROR.value]])
                )
                for query, result in zip(state["search_query"], state["web_research_result"])
            ]
        )
        formatted_prompt = deepresearch_prompts.answer_instructions.format(
            current_date=current_date,
            research_topic=get_research_topic(state["messages"]),  # type: ignore
            summaries=summaries,
        )

        # Use LLM client for final answer
        response = self.brain.think([{"role": "user", "content": formatted_prompt}])
        content = response.get("content", "")

        # Replace the short urls with the original urls and add all used urls to the sources_gathered
        unique_sources = []
        for source in state["sources_gathered"]:  # type: ignore
            if source.get("short_url", "") in content:  # type: ignore
                content = content.replace(
                    source["short_url"], source["value"]  # type: ignore
                )
                unique_sources.append(source)  # type: ignore

        return {
            "messages": [AIMessage(content=content)],
            "sources_gathered": unique_sources,
        }
